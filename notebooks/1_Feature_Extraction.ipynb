{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ebb6aca-121a-4436-bb92-ef3e67e01aed",
   "metadata": {
    "id": "4ebb6aca-121a-4436-bb92-ef3e67e01aed"
   },
   "source": [
    "# Installing Net2Brain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f61bc",
   "metadata": {},
   "source": [
    "<img src=\"workshops/data/Net2Brain_Logo.png\" width=\"25%\" />"
   ]
  },
  {
   "cell_type": "code",
   "id": "4ODrk2C7yMjr",
   "metadata": {
    "id": "4ODrk2C7yMjr"
   },
   "source": [
    "#!pip install -U git+https://github.com/cvai-roig-lab/Net2Brain"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12ecd847",
   "metadata": {},
   "source": [
    "# Step 1: Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "089bd358-1fd3-49da-8549-a309929dd434",
   "metadata": {
    "id": "089bd358-1fd3-49da-8549-a309929dd434"
   },
   "source": [
    "## Using `FeatureExtractor` with a model from Net2Brain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3114e293-a369-4634-a64b-057184cb4520",
   "metadata": {
    "id": "3114e293-a369-4634-a64b-057184cb4520"
   },
   "source": [
    "The FeatureExtractor class provides an interface for extracting features from a given model. When initializing this class, you can customize its behavior by setting various parameters:\n",
    "\n",
    "- `model` (required): The model from which you want to extract features. Either string in combination with a netset (next parameter), or a variable with a model-type.\n",
    "- `netset` (optional): The netset (collection of networks) that the model belongs to.\n",
    "- `device` (optional): The device on which to perform the computations, e.g., 'cuda' for GPU or 'cpu' for CPU. Default is None, which will use the device specified in the global PyTorch settings.\n",
    "- `pretrained` (optional): A boolean flag indicating whether to use a pretrained model (if available) or to initialize the model with random weights. Default is True, which means that a pretrained model will be used if possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ed1e591",
   "metadata": {},
   "source": [
    "- - -\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43a75bb8",
   "metadata": {},
   "source": [
    "First we need to a dataset to play around with. For that we will use the dataset by [Micheal F. Bonner (2017)](https://www.pnas.org/doi/full/10.1073/pnas.1618228114), which we can download using the `load_dataset` function"
   ]
  },
  {
   "cell_type": "code",
   "id": "73903734",
   "metadata": {},
   "source": [
    "from net2brain.utils.download_datasets import DatasetBonnerPNAS2017\n",
    "from pprint import pprint\n",
    "\n",
    "paths_bonner = DatasetBonnerPNAS2017.load_dataset()\n",
    "pprint(paths_bonner)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6bf25268",
   "metadata": {},
   "source": [
    "stimuli_path = paths_bonner[\"stimuli_path\"]\n",
    "roi_path = paths_bonner[\"roi_path\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c06970d",
   "metadata": {},
   "source": [
    "### Initating FeatureExtractor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2573aba3",
   "metadata": {},
   "source": [
    "\n",
    "To extract the activations of a pretrained model from a netset, you can use the FeatureExtractor class. First, you need to initialize the class by providing the name of the model and the name of the netset. You can find a suitable model and netset by exploring the taxonomy options available in the Net2Brain toolbox, as shown in the previous notebook \"0_Exploring_Net2Brain\". For instance, in the following example, we will use AlexNet from the standard netset."
   ]
  },
  {
   "cell_type": "code",
   "id": "83e48930-2eed-4551-8177-88dfe7a452d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "4c7a1d542761427791a841f9872d693f",
      "3587f70a049a47e1b344ce45039d527a",
      "e72c50f12dda4c56b9dfc0fea87bc6a9",
      "f1a4c2c8b3d94d0f821f0574b6434c85",
      "d28eed3c3fab499686cb90629e65078f",
      "21959f75a3a4472085d63d036fca70b2",
      "b30be957a0994291b112cd066b758cb5",
      "2a88a7872a0941c498a3cf260b559177",
      "940505c8262a4465aaaf109e8e2d5df8",
      "f432fb8b5cc54077911006ffe0b5bf12",
      "9247d2b379c64cd3926e669ed3be60c5"
     ]
    },
    "id": "83e48930-2eed-4551-8177-88dfe7a452d3",
    "outputId": "34a91416-caca-4b3d-9fa5-859d40ce99eb"
   },
   "source": [
    "from net2brain.feature_extraction import FeatureExtractor\n",
    "fx = FeatureExtractor(model='AlexNet', netset='Standard', device='cpu')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35ea8247",
   "metadata": {},
   "source": [
    "The `extract` method computes feature extraction from an image dataset. It takes in the following parameters:\n",
    "\n",
    "- `data_path` (required): The path to the images from which to extract the features. The images must be in JPEG or PNG format.\n",
    "- `save_path` (optional): The path to save the extracted features to. If None, the folder where the features are saved is named after the current date in the format \"{year}{month}{day}{hour}{minute}\".\n",
    "- `layers_to_extract` (optional): Either provide the list of layers to extract from the model, or a single string \n",
    "option from \"all\", \"top_level\", and \"json\", to extract all layers, only the top level blocks or predefined layers \n",
    "from a json file. Defaults to \"top_level\".\n",
    "- `consolidate_per_layer` (optional): The features are extracted image-wise. This is defaulted to true and will \n",
    "consolidate them per layer if not set to False. Defaults to True.\n",
    "- `dim_reduction` (optional): Type of dimensionality reduction to apply to the extracted features. Choose from `srp` (Sparse Random Projection) and `pca` (Principal Component Analysis). Defaults to None.\n",
    "- `n_samples_estim`: The number of samples used for estimating the dimensionality reduction. Defaults to 100.\n",
    "- `n_components` (optonal): Number of components for dimensionality reduction. If None, the number of components is estimated. Defaults to 10,000 (good value for SRP, not PCA).\n",
    "- `max_dim_allowed` (optional): The threshold over which the dimensionality reduction is applied. If None, it is always applied. Defaults to None."
   ]
  },
  {
   "cell_type": "code",
   "id": "39faa0f0",
   "metadata": {},
   "source": [
    "from net2brain.feature_extraction import FeatureExtractor\n",
    "\n",
    "fx = FeatureExtractor(model='AlexNet', netset='Standard', device='cpu')\n",
    "fx.extract(data_path=stimuli_path, save_path='AlexNet_Feat', consolidate_per_layer=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ZSbD3CBM94B-",
   "metadata": {
    "id": "ZSbD3CBM94B-"
   },
   "source": [
    "__Net2Brain__ chooses by default to extract from the top level blocks of the model. You can inspect \n",
    "which layers are selected by default by calling the `layers_to_extract` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "id": "426df22a-b261-4a7f-b94b-fd8cbdaeb030",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "426df22a-b261-4a7f-b94b-fd8cbdaeb030",
    "outputId": "62aa778b-455f-46c4-da52-8c344296a3bd"
   },
   "source": [
    "fx.layers_to_extract"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d86fce77",
   "metadata": {},
   "source": [
    "These are not all the layers that **can** be extracted. If you want to see all the layers that can possibly be extracted you you call `get_all_layers()`."
   ]
  },
  {
   "cell_type": "code",
   "id": "6619e401",
   "metadata": {},
   "source": [
    "fx.get_all_layers()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2064729b",
   "metadata": {},
   "source": [
    "If you wish to change the layers to be extracted you can add it to the `extract` function like with the parameter \n",
    "```\n",
    "fx.extract(..., layers_to_extract=[your_layers])\n",
    "```\n",
    "or layers_to_extract=\"all\" to extract from all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d95d74c158dda69",
   "metadata": {},
   "source": [
    "- - - \n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca22af19ef44afb",
   "metadata": {},
   "source": [
    "# Adding dimensionality reduction\n",
    "If you wish you can also reduce the dimensionality of the extracted feautures using:\n",
    "\n",
    "- `srp` (Sparse Random Projection)\n",
    "- `pca` (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "id": "d71b19f31ec77c3",
   "metadata": {},
   "source": [
    "from net2brain.feature_extraction import FeatureExtractor\n",
    "fx = FeatureExtractor(model='AlexNet', netset='Standard', device='cpu')\n",
    "fx.extract(data_path=stimuli_path, save_path='AlexNet_Feat_dim_red', dim_reduction=\"srp\", n_components=50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f173551f1a288878",
   "metadata": {},
   "source": [
    "If you want to save the original features to disk, but still want to reduce dimensionality for your analyses, this is \n",
    "also possible further down the pipeline when the features are loaded (see 2_RDM_Creation notebook). In that case, set\n",
    " `dim_reduction` to None in the extract function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2da7462edaa62d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655dccf2d11c450b",
   "metadata": {},
   "source": [
    "# Extracting Features from Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46913ac7341fab59",
   "metadata": {},
   "source": [
    "We have also added optionality to extract features from Large Language Models (LLMs) using .txt files. For this you just enter the path to your .txt files, in which each new line represents one new sentence.\n",
    "\n",
    "Since the feautures are saved per file, and since a .txt file might contain multiple sentences, you can `consolidate_per_txt_file()` in order to seperate each sentence into its own .npz file!"
   ]
  },
  {
   "cell_type": "code",
   "id": "c3c07d7f04289033",
   "metadata": {},
   "source": [
    "from net2brain.feature_extraction import FeatureExtractor\n",
    "\n",
    "\n",
    "extractor = FeatureExtractor(\"facebook/bart-base\", \"Huggingface\", device=\"cpu\")\n",
    "layers_to_extract = extractor.get_all_layers()\n",
    "print(layers_to_extract)\n",
    "\n",
    "extractor.extract(data_path=\"textinput_folder\", \n",
    "                  save_path=\"LLM_output\",\n",
    "                  consolidate_per_layer=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- - -\n",
    "\n",
    "- - -"
   ],
   "id": "c2c654d1d91d26f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Extracting Features from Audio Models\n",
   "id": "1eda33624db9ac37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can also extract features from audio models using any kind of audio data e.g. *.wav* or *.mp3* files. For this you just enter the path to your audio files. Currently, we provide a selection of CNN based models and Transformer based models. You can find the available models in the taxonomy notebook. For each model group there are mutliple models available, that differ in many design choices like the time window used or the number of layers. Please refer to the paper of the model for more details:\n",
    "* CNN-based models: [PANNS](https://arxiv.org/pdf/1912.10211)\n",
    "* Transformer-based models: [AST](https://arxiv.org/pdf/2104.01778)\n",
    "\n",
    "In the future we will also add more models to this list."
   ],
   "id": "d195329fd2b1d483"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from net2brain.feature_extraction import FeatureExtractor\n",
    "\n",
    "\n",
    "extractor = FeatureExtractor(model='PANNS_Cnn10', netset='Audio', device='cpu')\n",
    "print(extractor.layers_to_extract)\n",
    "\n",
    "extractor.extract(data_path=\"../net2brain/tests/audios\",\n",
    "                  save_path=\"Audio_output\",\n",
    "                  consolidate_per_layer=True)"
   ],
   "id": "17abe1ce072db65",
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6df000f0",
   "metadata": {},
   "source": [
    "- - - \n",
    "\n",
    "- - -"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "LX-73e5XGdnZ",
   "metadata": {
    "id": "LX-73e5XGdnZ"
   },
   "source": [
    "## Using `FeatureExtractor` with your own DNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "Agdb4-aySNw0",
   "metadata": {
    "id": "Agdb4-aySNw0"
   },
   "source": [
    "You can also incorporate your own custom model with __Net2Brain__. To do this, supply the `FeatureExtractor` with the following components:\n",
    "\n",
    "1. Your model\n",
    "2. An existing netset to fall back to (e.g. Standard, Clip, Pyvideo) when loading the data and applying standard \n",
    "functions.\n",
    "3. Optionally, your custom transform function (if not provided, standard ImageNet transformations will be used)\n",
    "4. Optionally your custom extraction function (if not provided, standard Torchextractor will be used)\n",
    "5. Optionally, your custom feature cleaner (if not provided, no cleaning will be done)\n",
    "6. The specific layers you want to extract features from"
   ]
  },
  {
   "cell_type": "code",
   "id": "bfcb9939-b1b0-4109-9f1f-d09cc3c5fa11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6e5efe2a9677426aa394c95d59889ada",
      "0bafe288788d46b58546f92a1c7f22bb",
      "a6b67d5dd3694f0ca1052590934c4658",
      "e58e3267d74341a8a096abb1f303266b",
      "d286815aa05d4551a5f43dcf3f10e590",
      "1b9f80c1125f44faab57cdbe47a13b98",
      "cf1bc2f7ae0f4bc281f2883b32e80a6c",
      "112f5f8813a94ed1be9a827d81fc277a",
      "fa2f89661fa34987869d15f910447e14",
      "c786c40315004074908af0247cbec3a4",
      "2ed025f53eeb4b04a66eab4a45cf419f"
     ]
    },
    "id": "bfcb9939-b1b0-4109-9f1f-d09cc3c5fa11",
    "outputId": "9d488345-e7a2-430f-9dcf-83853823bcee"
   },
   "source": [
    "from torchvision import models\n",
    "\n",
    "# Define a model\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)  # This one exists in the toolbox as well, it is just supposed to be an example!\n",
    "\n",
    "\n",
    "# Define extractor \n",
    "fx = FeatureExtractor(model=model, device='cpu')\n",
    "\n",
    "# Run extractor\n",
    "fx.extract(data_path=stimuli_path,  save_path='ResNet50_Feat', layers_to_extract=['layer1', 'layer2', 'layer3', 'layer4'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "baf55ed7c28694eb",
   "metadata": {},
   "source": [
    "Here an example with your custom functions. Make sure the parameters of your custom function match the ones here."
   ]
  },
  {
   "cell_type": "code",
   "id": "e5a8342b193f61f4",
   "metadata": {},
   "source": [
    "from torchvision import transforms as T\n",
    "from torchvision import transforms as trn\n",
    "import torchextractor as tx\n",
    "\n",
    "def my_preprocessor(image, model_name, device):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image (Union[Image.Image, List[Image.Image]]): A PIL Image or a list of PIL Images.\n",
    "        model_name (str): The name of the model, used to determine specific preprocessing if necessary.\n",
    "        device (str): The device to which the tensor should be transferred ('cuda' for GPU, 'cpu' for CPU).\n",
    "\n",
    "    Returns:\n",
    "        Union[torch.Tensor, List[torch.Tensor]]: The preprocessed image(s) as PyTorch tensor(s).\n",
    "    \"\"\"\n",
    "\n",
    "    transforms = trn.Compose([\n",
    "        trn.Resize((224, 224)),\n",
    "        trn.ToTensor(),\n",
    "        trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    img_tensor = transforms(image).unsqueeze(0)\n",
    "    if device == 'cuda':\n",
    "        img_tensor = img_tensor.cuda()\n",
    "\n",
    "    return img_tensor\n",
    "\n",
    "\n",
    "def my_extactor(preprocessed_data, layers_to_extract, model):\n",
    "    # Create a extractor instance\n",
    "    extractor_model = tx.Extractor(model, layers_to_extract)\n",
    "    \n",
    "    # Extract actual features\n",
    "    _, features = extractor_model(preprocessed_data)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def my_cleaner(features):\n",
    "    return features\n",
    "\n",
    "\n",
    "# Define a model\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)  # This one exists in the toolbox as well, it is just supposed to be an example!\n",
    "\n",
    "## Define extractor (Note: NO NETSET NEEDED HERE)\n",
    "fx = FeatureExtractor(model=model, device='cpu', preprocessor=my_preprocessor, feature_cleaner=my_cleaner, extraction_function=my_extactor)\n",
    "\n",
    "# Run extractor\n",
    "fx.extract(stimuli_path, layers_to_extract=['layer1', 'layer2', 'layer3', 'layer4'])"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "N2B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2143370df03d7e8d8afb3cb32a8181ea228f5a6f13a304f592978240ae0036e6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0bafe288788d46b58546f92a1c7f22bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b9f80c1125f44faab57cdbe47a13b98",
      "placeholder": "​",
      "style": "IPY_MODEL_cf1bc2f7ae0f4bc281f2883b32e80a6c",
      "value": "100%"
     }
    },
    "112f5f8813a94ed1be9a827d81fc277a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b9f80c1125f44faab57cdbe47a13b98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21959f75a3a4472085d63d036fca70b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a88a7872a0941c498a3cf260b559177": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ed025f53eeb4b04a66eab4a45cf419f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3587f70a049a47e1b344ce45039d527a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21959f75a3a4472085d63d036fca70b2",
      "placeholder": "​",
      "style": "IPY_MODEL_b30be957a0994291b112cd066b758cb5",
      "value": "100%"
     }
    },
    "4c7a1d542761427791a841f9872d693f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3587f70a049a47e1b344ce45039d527a",
       "IPY_MODEL_e72c50f12dda4c56b9dfc0fea87bc6a9",
       "IPY_MODEL_f1a4c2c8b3d94d0f821f0574b6434c85"
      ],
      "layout": "IPY_MODEL_d28eed3c3fab499686cb90629e65078f"
     }
    },
    "6e5efe2a9677426aa394c95d59889ada": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0bafe288788d46b58546f92a1c7f22bb",
       "IPY_MODEL_a6b67d5dd3694f0ca1052590934c4658",
       "IPY_MODEL_e58e3267d74341a8a096abb1f303266b"
      ],
      "layout": "IPY_MODEL_d286815aa05d4551a5f43dcf3f10e590"
     }
    },
    "9247d2b379c64cd3926e669ed3be60c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "940505c8262a4465aaaf109e8e2d5df8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a6b67d5dd3694f0ca1052590934c4658": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_112f5f8813a94ed1be9a827d81fc277a",
      "max": 244408911,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fa2f89661fa34987869d15f910447e14",
      "value": 244408911
     }
    },
    "b30be957a0994291b112cd066b758cb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c786c40315004074908af0247cbec3a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf1bc2f7ae0f4bc281f2883b32e80a6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d286815aa05d4551a5f43dcf3f10e590": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d28eed3c3fab499686cb90629e65078f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e58e3267d74341a8a096abb1f303266b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c786c40315004074908af0247cbec3a4",
      "placeholder": "​",
      "style": "IPY_MODEL_2ed025f53eeb4b04a66eab4a45cf419f",
      "value": " 233M/233M [00:02&lt;00:00, 78.5MB/s]"
     }
    },
    "e72c50f12dda4c56b9dfc0fea87bc6a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a88a7872a0941c498a3cf260b559177",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_940505c8262a4465aaaf109e8e2d5df8",
      "value": 102530333
     }
    },
    "f1a4c2c8b3d94d0f821f0574b6434c85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f432fb8b5cc54077911006ffe0b5bf12",
      "placeholder": "​",
      "style": "IPY_MODEL_9247d2b379c64cd3926e669ed3be60c5",
      "value": " 97.8M/97.8M [00:00&lt;00:00, 140MB/s]"
     }
    },
    "f432fb8b5cc54077911006ffe0b5bf12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa2f89661fa34987869d15f910447e14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce0ad1b7",
   "metadata": {},
   "source": [
    "# Installing Net2Brain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00329106",
   "metadata": {},
   "source": [
    "<img src=\"workshops/data/Net2Brain_Logo.png\" width=\"25%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ODrk2C7yMjr",
   "metadata": {
    "id": "4ODrk2C7yMjr"
   },
   "outputs": [],
   "source": [
    "#!pip install -U git+https://github.com/cvai-roig-lab/Net2Brain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc923836-dbd6-448a-81a0-1b67ce9afa80",
   "metadata": {
    "id": "dc923836-dbd6-448a-81a0-1b67ce9afa80"
   },
   "source": [
    "__Net2Brain__ allows you to use one of over 600 Deep Neural Networks (DNNs) for your experiments comparing human brain activity with the activations of artificial neural networks. The DNNs in __Net2Brain__ are obtained from what we call different _netsets_, which are libraries that provide different pretrained models. \n",
    "\n",
    "__Net2Brain__ provides access to the following _netsets_:\n",
    "- [Standard torchvision](https://pytorch.org/vision/stable/models.html) (`Pytorch`).\n",
    "This netset is a collection of the torchvision models including models for image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection, video classification, and optical flow.\n",
    "- [Timm](https://github.com/rwightman/pytorch-image-models#models) (`Timm`). \n",
    "A deep-learning library created by Ross Wightman that contains a collection of state-of-the-art computer vision models.\n",
    "- [PyTorch Hub](https://pytorch.org/docs/stable/hub.html) (`Torchhub`). \n",
    "These models are accessible through the torch.hub API and are trained for different visual tasks. They are not included in the torchvision module.\n",
    "- [PyTorch Video](https://pytorch.org/docs/stable/hub.html) (`Pyvideo`). \n",
    "Offers models for video analysis, including action recognition and motion classification.\n",
    "- [Unet](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/) (`Unet`). \n",
    "Unet also is available through the torch.hub.API and is trained for abnormality segmentation in brain MRI.\n",
    "- [Taskonomy](https://github.com/StanfordVL/taskonomy) (`Taskonomy`). A set of networks trained for different visual tasks, like Keypoint-Detection, Depth-Estimation, Reshading, etc. The initial idea for these networks was to find relationships between different visual tasks.\n",
    "- [Slowfast](https://github.com/facebookresearch/pytorchvideo) (`Pyvideo`). \n",
    "These models are state-of-the-art video classification models trained on the Kinetics 400 dataset, acessible through the torch.hub API.\n",
    "- [CLIP](https://github.com/openai/CLIP) (`Clip`). \n",
    "CLIP (Contrastive Language-Image Pre-Training) is a vision+language multimodal neural network trained on a variety of (image, text) pairs.\n",
    "- [CorNet](https://github.com/dicarlolab/CORnet) (`Cornet`). \n",
    "A set of neural networks whose structure is supposed to resemble the one of the ventral visual pathway and therefore implements more recurrent connections that are commonplace in the VVS.\n",
    "- [Huggingface](https://huggingface.co/) (`Huggingface`). \n",
    "Features a broad range of advanced language models that deal with text-input.\n",
    "- [Yolo](https://github.com/ultralytics/yolov5) (`Yolo`). \n",
    "Includes fast, accurate YOLOv5 models for real-time object detection in images and video streams.\n",
    "- **Toolbox** (`Toolbox`). \n",
    "A set of networks that are implemented within Net2Brain.\n",
    "- **Audio** (`Audio`).\n",
    "A set of networks that are trained for audio tasks.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "**Net2Brain** consists of 4 main parts:\n",
    "1. **Feature Extraction**\n",
    "  > Handles input in the form of images, videos or text and extracts relevant features for analysis and saves them into .npz files.\n",
    "2. **Representational Dissimilarity Matrix (RDM) Creation**\n",
    "> Utilizes numpy arrays (.npz files) from the feature extraction process to create RDMs that quantify dissimilarities between data representations with different distance metrics.\n",
    "3. **Evaluation**\n",
    "> Provides a comprehensive suite of evaluation methods including Linear Encoding, Representational Similarity Analysis (RSA), and more, to assess and compare model performance\n",
    "4. **Plotting**\n",
    "> Offers advanced visualization tools to graphically display the results of various analyses, enhancing interpretability and presentation of findings.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50f0236f",
   "metadata": {},
   "source": [
    "# Step 0: Exploring the Toolbox - Model Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from net2brain.taxonomy import show_all_architectures\n",
    "from net2brain.taxonomy import show_all_netsets\n",
    "from net2brain.taxonomy import show_taxonomy\n",
    "from net2brain.taxonomy import print_netset_models\n",
    "\n",
    "from net2brain.taxonomy import find_model_like_name\n",
    "from net2brain.taxonomy import find_model_by_dataset\n",
    "from net2brain.taxonomy import find_model_by_training_method\n",
    "from net2brain.taxonomy import find_model_by_visual_task\n",
    "from net2brain.taxonomy import find_model_by_custom"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61d656e9",
   "metadata": {},
   "source": [
    "To view a list of all available models along with the information on which netset they belong to, you can use the `print_all_models()` function to print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aaeac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_architectures()\n",
    "show_all_netsets()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a42a973d",
   "metadata": {},
   "source": [
    "You can also inspect the models available from a particular _netset_ using the function `print_netset_models()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_netset_models('Standard')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f7a4688",
   "metadata": {},
   "source": [
    "Or you can find a model by its name using the function `find_model_like()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_model_like_name('ResNet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eaf8d680",
   "metadata": {},
   "source": [
    "We also offer a comprehensive model taxonomy to help you find the most suitable model for your study. Each model in our toolbox has distinct attributes that cater to various research requirements. To facilitate your selection process, we provide a taxonomic overview of the models available.\n",
    "\n",
    "To see the available attributes, use the show_taxonomy function. You can then search for a model based on one or more attributes using the following functions:\n",
    "\n",
    "- `find_model_by_dataset(attributes)`\n",
    "- `find_model_by_training_method(attributes)`\n",
    "- `find_model_by_visual_task(attributes)`\n",
    "- `find_model_by_custom([attributes], model_name)`\n",
    "\n",
    "This taxonomy system is designed to help you easily identify and choose the most appropriate model for your research needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ecd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_taxonomy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dd902f2",
   "metadata": {},
   "source": [
    "The `find_model_by_dataset(attributes)` function enables you to search for models associated with a specific dataset, such as 'ImageNet', 'ImageNet 22K', or 'COCO'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5091b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_model_by_dataset(\"Taskonomy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1056326a",
   "metadata": {},
   "source": [
    "The `find_model_by_training_method(attributes)` function helps you discover models based on their training methodology, such as 'Supervised', 'Jigsaw', or 'NPID'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8fea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_model_by_training_method(\"SimCLR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c276d65",
   "metadata": {},
   "source": [
    "The `find_model_by_visual_task(attributes)` function allows you to search for models specifically trained for a particular visual task, such as 'Object Detection', 'Panoptic Segmentation', or 'Semantic Segmentation'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9977a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_model_by_visual_task(\"Panoptic Segmentation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48192678",
   "metadata": {},
   "source": [
    "The `find_model_by_custom([attributes], model_name)` function enables you to search for models based on a combination of the attributes mentioned above. You can provide a list of attributes to filter the models, and optionally specify a particular model name to further refine your search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a81805",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_model_by_custom([\"COCO\", \"Object Detection\"], model_name=\"fpn\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a56e796c",
   "metadata": {},
   "source": [
    "# Downloading Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e432167f",
   "metadata": {},
   "source": [
    "To quickly test the Net2Brain toolbox, we provide some datasets that you can download and play around with. To download them, you can use the following code.\n",
    "It will return a dictionary containing all available datapaths.\n",
    "\n",
    "\n",
    "The available datasets are:\n",
    "- The 78Images-Dataset from [Algonauts2019 Challenge Training Set A](http://algonauts.csail.mit.edu/2019/download.html)\n",
    "- The 92Images-Dataset from [Algonauts2019 Challenge Test Set](http://algonauts.csail.mit.edu/2019/download.html)\n",
    "- The bonnerpnas2017-Dataset from [Micheal F. Bonner et. al](https://www.pnas.org/doi/full/10.1073/pnas.1618228114)\n",
    "- The NSD-Dataset (Algonauts Challenge) from [EJ Allen et. al](https://www.nature.com/articles/s41593-021-00962-x)\n",
    "- A subset of the NSD-Dataset with the 872 images that all participants have seen from [EJ Allen et. al](https://www.nature.com/articles/s41593-021-00962-x)\n",
    "- The BoldMoment-Dataset from [Lahner et al.](https://www.nature.com/articles/s41467-024-50310-3), which provides fMRI responses to naturalistic video clips, covering a wide range of semantic categories to study how the brain processes dynamic visual scenes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3fed26",
   "metadata": {},
   "source": [
    "To list all available datasets you can use `list_available_datasets()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from net2brain.utils.download_datasets import list_available_datasets\n",
    "list_available_datasets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f47b77c",
   "metadata": {},
   "source": [
    "In order to download the dataset use the `load_dataset()` function of each dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from net2brain.utils.download_datasets import DatasetNSD_872, Dataset78images, Dataset92images, DatasetBonnerPNAS2017, DatasetAlgonauts_NSD, DatasetBoldMoments\n",
    "from pprint import pprint\n",
    "\n",
    "paths_78 = Dataset78images.load_dataset()\n",
    "paths_92 = Dataset92images.load_dataset()\n",
    "paths_bonner = DatasetBonnerPNAS2017.load_dataset()\n",
    "paths_NSD = DatasetNSD_872.load_dataset()\n",
    "paths_Algonauts = DatasetAlgonauts_NSD.load_dataset()\n",
    "paths_BoldMoments = DatasetBoldMoments.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc5a1d8",
   "metadata": {},
   "source": [
    "This will return a dictionary containig all available datafolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5495d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Paths for 78images-Dataset:\")\n",
    "pprint(paths_78)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Paths for 92images-Dataset:\")\n",
    "pprint(paths_92)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Paths for BonnerPNAS2017-Dataset:\")\n",
    "pprint(paths_bonner)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Paths for NSD-Dataset:\")\n",
    "pprint(paths_NSD)\n",
    "\n",
    "print(\"Paths for Algonauts-Dataset:\")\n",
    "pprint(paths_Algonauts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ab87d",
   "metadata": {},
   "source": [
    "In order to get the stimuli data and roi data of the `78images`-Dataset you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a553e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_path = paths_78[\"stimuli_path\"]\n",
    "roi_path = paths_78[\"roi_path\"]\n",
    "\n",
    "print(stimuli_path)\n",
    "print(roi_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238ce364",
   "metadata": {},
   "source": [
    "# Datasets with additional functionality (NSD & Algonauts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b8002",
   "metadata": {},
   "source": [
    "## The NSD-Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441bbc83",
   "metadata": {},
   "source": [
    "The Natural Scenes Dataset (NSD) is a valuable resource that shares its roots with the COCO dataset, making it a rich dataset for visual neuroscience studies. Recognizing the potential of leveraging COCO's extensive annotations and images, we've developed a suite of functions within the `Net2Brain` toolkit to facilitate seamless interchange between NSD and COCO identifiers, images, segmentations and captions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d311a",
   "metadata": {},
   "source": [
    "### Key Functions Overview\n",
    "\n",
    "The DatasetNSD class offers a range of functions designed to bridge NSD and COCO, enhancing the utility of the NSD dataset for comprehensive visual studies:\n",
    "\n",
    "* **Image Downloads**: Access original COCO images directly from the NSD context.\n",
    "* **ID Conversion**: Switch between NSD and COCO identifiers.\n",
    "* **Segmentation Masks**: Obtain COCO segmentation masks corresponding to NSD images.\n",
    "* **Caption Downloads**: Obtain COCO original caption to each downloaded image.\n",
    "* **Image and Mask Manipulation**: Crop and rename files for consistency with NSD conventions.\n",
    "* **Visualization**: Display the images along with their segmentaion masks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f253d1c9",
   "metadata": {},
   "source": [
    "#### Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a6264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from net2brain.utils.download_datasets import DatasetNSD_872, DatasetAlgonauts_NSD\n",
    "\n",
    "# Load dataset paths and initialize the DatasetNSD object\n",
    "NSD_dataset = DatasetNSD_872()  # or use DatasetAlgonauts_NSD\n",
    "paths = NSD_dataset.load_dataset(path=\"my_save_path\")\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b057e9",
   "metadata": {},
   "source": [
    "### ID Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973af1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NSD ID to COCO ID and vice versa\n",
    "print(NSD_dataset.NSDtoCOCO(\"02950\"))  # NSD to COCO\n",
    "print(NSD_dataset.COCOtoNSD(\"262145\"))  # COCO to NSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697222ba",
   "metadata": {},
   "source": [
    "#### Downloading COCO Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download COCO images corresponding to NSD images\n",
    "NSD_dataset.Download_COCO_Images(nsd_image_folder=\"NSD Dataset/NSD_872_images\",\n",
    "                                 target_folder=\"NSD Dataset/coco_images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9560f6",
   "metadata": {},
   "source": [
    "#### Downloading COCO Segmentation Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963811f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download COCO segmentation masks for NSD images\n",
    "NSD_dataset.Download_COCO_Segmentation_Masks(nsd_image_folder=\"NSD Dataset/NSD_872_images\",\n",
    "                                             target_folder=\"NSD Dataset/coco_masks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da0f2b",
   "metadata": {},
   "source": [
    "#### Downloading COCO Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dca5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download COCO images corresponding to NSD images\n",
    "NSD_dataset.Download_COCO_Captions(nsd_image_folder=\"NSD Dataset/NSD_872_images\",\n",
    "                                 target_folder=\"NSD Dataset/coco_captions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb00668",
   "metadata": {},
   "source": [
    "#### Visualizing Images and Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e95f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize an image and its segmentation mask side by side\n",
    "NSD_dataset.Visualize(\"NSD Dataset/coco_images\", \"NSD Dataset/coco_masks\", \"03171\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed33a4",
   "metadata": {},
   "source": [
    "#### Cropping COCO Images to NSD aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop COCO images/masks to fit NSD dimensions\n",
    "NSD_dataset.Crop_COCO_to_NSD(source_folder=\"NSD Dataset/coco_images\",\n",
    "                             target_folder=\"NSD Dataset/coco_images\")\n",
    "\n",
    "NSD_dataset.Crop_COCO_to_NSD(source_folder=\"NSD Dataset/coco_masks\",\n",
    "                             target_folder=\"NSD Dataset/coco_masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb55f6",
   "metadata": {},
   "source": [
    "#### Rename for COCO Images for NSD compability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a3718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename files for consistent NSD naming conventions\n",
    "NSD_dataset.RenameToNSD(\"NSD Dataset/coco_images\")\n",
    "NSD_dataset.RenameToNSD(\"NSD Dataset/coco_masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f139de",
   "metadata": {},
   "source": [
    "Done all that you should have in the folders `coco_images` and `coco_masks` now the coco images with their NSD name, in the NSD aspect ratio, alogn with their segmentation masks. Of course you can visualize them as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533de218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize an image and its segmentation mask side by side\n",
    "NSD_dataset.Visualize(\"NSD Dataset/coco_images\", \"NSD Dataset/coco_masks\", \"02950\") ## Attention: These are NSD IDs now!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f73a607",
   "metadata": {},
   "source": [
    "There is an additional function that renames from the Algonauts naming convention to the NSD naming convention:\n",
    "\n",
    "`train-9389_nsd-69513.png -> 69513.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6982026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NSD_dataset.RenameAlgonautsToNSD(\"path/to/Algonauts\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "N2B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2143370df03d7e8d8afb3cb32a8181ea228f5a6f13a304f592978240ae0036e6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0bafe288788d46b58546f92a1c7f22bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b9f80c1125f44faab57cdbe47a13b98",
      "placeholder": "​",
      "style": "IPY_MODEL_cf1bc2f7ae0f4bc281f2883b32e80a6c",
      "value": "100%"
     }
    },
    "112f5f8813a94ed1be9a827d81fc277a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b9f80c1125f44faab57cdbe47a13b98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21959f75a3a4472085d63d036fca70b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a88a7872a0941c498a3cf260b559177": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ed025f53eeb4b04a66eab4a45cf419f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3587f70a049a47e1b344ce45039d527a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21959f75a3a4472085d63d036fca70b2",
      "placeholder": "​",
      "style": "IPY_MODEL_b30be957a0994291b112cd066b758cb5",
      "value": "100%"
     }
    },
    "4c7a1d542761427791a841f9872d693f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3587f70a049a47e1b344ce45039d527a",
       "IPY_MODEL_e72c50f12dda4c56b9dfc0fea87bc6a9",
       "IPY_MODEL_f1a4c2c8b3d94d0f821f0574b6434c85"
      ],
      "layout": "IPY_MODEL_d28eed3c3fab499686cb90629e65078f"
     }
    },
    "6e5efe2a9677426aa394c95d59889ada": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0bafe288788d46b58546f92a1c7f22bb",
       "IPY_MODEL_a6b67d5dd3694f0ca1052590934c4658",
       "IPY_MODEL_e58e3267d74341a8a096abb1f303266b"
      ],
      "layout": "IPY_MODEL_d286815aa05d4551a5f43dcf3f10e590"
     }
    },
    "9247d2b379c64cd3926e669ed3be60c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "940505c8262a4465aaaf109e8e2d5df8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a6b67d5dd3694f0ca1052590934c4658": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_112f5f8813a94ed1be9a827d81fc277a",
      "max": 244408911,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fa2f89661fa34987869d15f910447e14",
      "value": 244408911
     }
    },
    "b30be957a0994291b112cd066b758cb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c786c40315004074908af0247cbec3a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf1bc2f7ae0f4bc281f2883b32e80a6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d286815aa05d4551a5f43dcf3f10e590": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d28eed3c3fab499686cb90629e65078f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e58e3267d74341a8a096abb1f303266b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c786c40315004074908af0247cbec3a4",
      "placeholder": "​",
      "style": "IPY_MODEL_2ed025f53eeb4b04a66eab4a45cf419f",
      "value": " 233M/233M [00:02&lt;00:00, 78.5MB/s]"
     }
    },
    "e72c50f12dda4c56b9dfc0fea87bc6a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a88a7872a0941c498a3cf260b559177",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_940505c8262a4465aaaf109e8e2d5df8",
      "value": 102530333
     }
    },
    "f1a4c2c8b3d94d0f821f0574b6434c85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f432fb8b5cc54077911006ffe0b5bf12",
      "placeholder": "​",
      "style": "IPY_MODEL_9247d2b379c64cd3926e669ed3be60c5",
      "value": " 97.8M/97.8M [00:00&lt;00:00, 140MB/s]"
     }
    },
    "f432fb8b5cc54077911006ffe0b5bf12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa2f89661fa34987869d15f910447e14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

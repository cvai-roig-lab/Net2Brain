{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ARCHITECTURE COUNTS ===\n",
      "CNN: 522\n",
      "Transformer-based: 13\n",
      "Vision Transformer: 80\n",
      "Swin-Transformer: 10\n",
      "MLP-Mixer: 25\n",
      "Multimodal: 5\n",
      "\n",
      "=== TASK COUNTS ===\n",
      "Image Classification: 520\n",
      "Object Detection: 32\n",
      "Panoptic Segmentation: 4\n",
      "Semantic Segmentation: 1\n",
      "Instance Segmentation: 24\n",
      "Keypoint Detection: 4\n",
      "Pose Estimation: 0\n",
      "Video Classification: 6\n",
      "Natural Language Processing: 6\n",
      "Audio Tagging: 29\n",
      "Various visual tasks: 23\n",
      "Combined Segmentation: 29\n",
      "\n",
      "=== TRAINING PARADIGM COUNTS ===\n",
      "Supervised: 600\n",
      "Jigsaw: 7\n",
      "NPID: 2\n",
      "RotNet: 2\n",
      "Clusterfit: 1\n",
      "Deepcluster: 3\n",
      "SimCLR: 5\n",
      "SwAV: 7\n",
      "MoCo: 1\n",
      "CLIP: 5\n",
      "Combined Self-supervised: 28\n",
      "\n",
      "Total models: 649\n",
      "\n",
      "===== TABLE 1: Hierarchical Summary Table =====\n",
      " Architecture Type  Count                                                                                        Examples                     Typical Applications in Neuroscience\n",
      "               CNN    522                                                                     AlexNet, ResNet18, ResNet34               Visual cortex modeling, object recognition\n",
      " Transformer-based     13                                       albert-base-v2, kakaobrain/align-base, facebook/bart-base     Complex scene understanding, hierarchical processing\n",
      "Vision Transformer     80                                                    adv_inception_v3, cait_m36_384, cait_m48_448   Visual attention mechanisms, global feature extraction\n",
      "  Swin-Transformer     10 swin_base_patch4_window7_224, swin_base_patch4_window7_224_in22k, swin_base_patch4_window12_384 Hierarchical visual processing, local-global integration\n",
      "         MLP-Mixer     25                                                      gmixer_24_224, gmlp_s16_224, mixer_b16_224           Visual feature extraction without convolutions\n",
      "        Multimodal      5                                                                         RN50, RN101, ViT-B_-_32                          Cross-modal integration studies\n",
      "\n",
      "\n",
      "===== TABLE 2: Training Paradigm Distribution =====\n",
      "Training Paradigm               Dataset Examples  Count                                              Neuroscientific Relevance\n",
      "       Supervised                 ImageNet, COCO    600                                  Comparison to human category learning\n",
      "  Self-supervised             SimCLR, MoCo, SwAV     28 Unsupervised feature representation similar to developmental processes\n",
      "Multimodal (CLIP)              CLIP (image-text)      5                                        Cross-modal integration studies\n",
      "    Task-specific Object detection, segmentation     61                                 Specialized visual processing pathways\n",
      "           Jigsaw                                     7                                                                       \n",
      "             NPID                                     2                                                                       \n",
      "           RotNet                                     2                                                                       \n",
      "       Clusterfit                                     1                                                                       \n",
      "           SimCLR                                     5                                                                       \n",
      "             SwAV                                     7                                                                       \n",
      "             MoCo                                     1                                                                       \n",
      "\n",
      "\n",
      "===== TABLE 3: Task-Based Selection Criteria =====\n",
      "              Task Category  # Models                                                                                                                                                                                                                                                                                                                                         Key Architectures                 Neuroscientific Application\n",
      "       Image Classification       520                                                                                                                                                                                                                                                                                                                               AlexNet, ResNet18, ResNet34   Ventral visual stream, object recognition\n",
      "           Object Detection        32                                                                                                                                                                                                                                                                                                                   cspdarknet53, cspresnet50, cspresnext50 Attentional mechanisms, object localization\n",
      "      Panoptic Segmentation         4                                                                                                                                                                                      COCO-PanopticSegmentation_-_panoptic_fpn_R_50_1x.yaml, COCO-PanopticSegmentation_-_panoptic_fpn_R_50_3x.yaml, COCO-PanopticSegmentation_-_panoptic_fpn_R_101_3x.yaml    Scene parsing, figure-ground segregation\n",
      "      Semantic Segmentation         1                                                                                                                                                                                                                                                                                                                                              SceneParsing    Scene parsing, figure-ground segregation\n",
      "      Instance Segmentation        24                                                                                                                                                                                     COCO-InstanceSegmentation_-_mask_rcnn_R_50_C4_1x.yaml, COCO-InstanceSegmentation_-_mask_rcnn_R_50_DC5_1x.yaml, COCO-InstanceSegmentation_-_mask_rcnn_R_50_FPN_1x.yaml    Scene parsing, figure-ground segregation\n",
      "         Keypoint Detection         4                                                                                                                                                                                                 COCO-Keypoints_-_keypoint_rcnn_R_50_FPN_1x.yaml, COCO-Keypoints_-_keypoint_rcnn_R_101_FPN_3x.yaml, COCO-Keypoints_-_keypoint_rcnn_X_101_32x8d_FPN_3x.yaml                Biological motion perception\n",
      "       Video Classification         6                                                                                                                                                                                                                                                                                                                     slow_r50, slowfast_r101, slowfast_r50     Motion processing, temporal integration\n",
      "Natural Language Processing         6                                                                                                                                                                                                                                                                                                 albert-base-v2, kakaobrain/align-base, facebook/bart-base                                           ?\n",
      "              Audio Tagging        29                                                                                                                                                                                                                                                                                                              PANNS_Cnn10, PANNS_Cnn14_16k, PANNS_Cnn14_8k                Auditory processing pathways\n",
      "       Various visual tasks        23                                                                                                                                                                                                                                                                                                                     autoencoding, curvature, class_object                                       other\n",
      "     All Segmentation Tasks        29 COCO-PanopticSegmentation_-_panoptic_fpn_R_50_1x.yaml, COCO-PanopticSegmentation_-_panoptic_fpn_R_50_3x.yaml, COCO-PanopticSegmentation_-_panoptic_fpn_R_101_3x.yaml, SceneParsing, COCO-InstanceSegmentation_-_mask_rcnn_R_50_C4_1x.yaml, COCO-InstanceSegmentation_-_mask_rcnn_R_50_DC5_1x.yaml, COCO-InstanceSegmentation_-_mask_rcnn_R_50_FPN_1x.yaml    Scene parsing, figure-ground segregation\n",
      "\n",
      "\n",
      "===== LATEX TABLE 1 =====\n",
      "\\begin{table}[htbp]\n",
      "\\centering\n",
      "\\caption{Distribution of models by architecture type}\n",
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "Architecture Type & Count & Examples & Typical Applications in Neuroscience \\\\\n",
      "\\midrule\n",
      "CNN & 522 & AlexNet, ResNet18, ResNet34 & Visual cortex modeling, object recognition \\\\\n",
      "Transformer-based & 13 & albert-base-v2, kakaobrain/align-base, facebook/bart-base & Complex scene understanding, hierarchical processing \\\\\n",
      "Vision Transformer & 80 & adv_inception_v3, cait_m36_384, cait_m48_448 & Visual attention mechanisms, global feature extraction \\\\\n",
      "Swin-Transformer & 10 & swin_base_patch4_window7_224, swin_base_patch4_window7_224_in22k, swin_base_patch4_window12_384 & Hierarchical visual processing, local-global integration \\\\\n",
      "MLP-Mixer & 25 & gmixer_24_224, gmlp_s16_224, mixer_b16_224 & Visual feature extraction without convolutions \\\\\n",
      "Multimodal & 5 & RN50, RN101, ViT-B_-_32 & Cross-modal integration studies \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "===== LATEX TABLE 2 =====\n",
      "\\begin{table}[htbp]\n",
      "\\centering\n",
      "\\caption{Distribution of models by training paradigm}\n",
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "Training Paradigm & Dataset Examples & Count & Neuroscientific Relevance \\\\\n",
      "\\midrule\n",
      "Supervised & ImageNet, COCO & 600 & Comparison to human category learning \\\\\n",
      "Self-supervised & SimCLR, MoCo, SwAV & 28 & Unsupervised feature representation similar to developmental processes \\\\\n",
      "Multimodal (CLIP) & CLIP (image-text) & 5 & Cross-modal integration studies \\\\\n",
      "Task-specific & Object detection, segmentation & 61 & Specialized visual processing pathways \\\\\n",
      "  Jigsaw &  & 7 &  \\\\\n",
      "  NPID &  & 2 &  \\\\\n",
      "  RotNet &  & 2 &  \\\\\n",
      "  Clusterfit &  & 1 &  \\\\\n",
      "  SimCLR &  & 5 &  \\\\\n",
      "  SwAV &  & 7 &  \\\\\n",
      "  MoCo &  & 1 &  \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "===== LATEX TABLE 3 =====\n",
      "\\begin{table}[htbp]\n",
      "\\centering\n",
      "\\caption{Distribution of models by task category}\n",
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "Task Category & # Models & Key Architectures & Neuroscientific Application \\\\\n",
      "\\midrule\n",
      "Image Classification & 520 & AlexNet, ResNet18, ResNet34 & Ventral visual stream, object recognition \\\\\n",
      "Object Detection & 32 & cspdarknet53, cspresnet50, cspresnext50 & Attentional mechanisms, object localization \\\\\n",
      "Panoptic Segmentation & 4 & COCO-PanopticSegmentation_-_panoptic_fpn_R_50_1x.yaml, COCO-PanopticSegmentation_-_panoptic_fpn_R_50_3x.yaml, COCO-PanopticSegmentation_-_panoptic_fpn_R_101_3x.yaml & Scene parsing, figure-ground segregation \\\\\n",
      "Semantic Segmentation & 1 & SceneParsing & Scene parsing, figure-ground segregation \\\\\n",
      "Instance Segmentation & 24 & COCO-InstanceSegmentation_-_mask_rcnn_R_50_C4_1x.yaml, COCO-InstanceSegmentation_-_mask_rcnn_R_50_DC5_1x.yaml, COCO-InstanceSegmentation_-_mask_rcnn_R_50_FPN_1x.yaml & Scene parsing, figure-ground segregation \\\\\n",
      "Keypoint Detection & 4 & COCO-Keypoints_-_keypoint_rcnn_R_50_FPN_1x.yaml, COCO-Keypoints_-_keypoint_rcnn_R_101_FPN_3x.yaml, COCO-Keypoints_-_keypoint_rcnn_X_101_32x8d_FPN_3x.yaml & Biological motion perception \\\\\n",
      "Video Classification & 6 & slow_r50, slowfast_r101, slowfast_r50 & Motion processing, temporal integration \\\\\n",
      "Natural Language Processing & 6 & albert-base-v2, kakaobrain/align-base, facebook/bart-base & ? \\\\\n",
      "Audio Tagging & 29 & PANNS_Cnn10, PANNS_Cnn14_16k, PANNS_Cnn14_8k & Auditory processing pathways \\\\\n",
      "Various visual tasks & 23 & autoencoding, curvature, class_object & other \\\\\n",
      "All Segmentation Tasks & 29 & COCO-PanopticSegmentation_-_panoptic_fpn_R_50_1x.yaml, COCO-PanopticSegmentation_-_panoptic_fpn_R_50_3x.yaml, COCO-PanopticSegmentation_-_panoptic_fpn_R_101_3x.yaml, SceneParsing, COCO-InstanceSegmentation_-_mask_rcnn_R_50_C4_1x.yaml, COCO-InstanceSegmentation_-_mask_rcnn_R_50_DC5_1x.yaml, COCO-InstanceSegmentation_-_mask_rcnn_R_50_FPN_1x.yaml & Scene parsing, figure-ground segregation \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('net2brain/architectures/taxonomy.csv')\n",
    "\n",
    "# Identify where actual data begins (after header rows)\n",
    "data_start = 1  # Based on your sample, data starts from row index 1\n",
    "\n",
    "# Get the actual data rows\n",
    "model_data = data.iloc[data_start:].copy()\n",
    "\n",
    "# Function to check if a value represents an 'x' marking\n",
    "def is_marked(val):\n",
    "    return val == 'x' or val == 'X'\n",
    "\n",
    "# Function to count models with a specific attribute\n",
    "def count_by_attribute(column_name):\n",
    "    return model_data[column_name].apply(is_marked).sum()\n",
    "\n",
    "# Function to get example models for a category\n",
    "def get_example_models(column_name, limit=3):\n",
    "    mask = model_data[column_name].apply(is_marked)\n",
    "    examples = model_data[mask]['Model'].head(limit)\n",
    "    if len(examples) == 0:\n",
    "        return \"N/A\"\n",
    "    return \", \".join(examples.astype(str).values)\n",
    "\n",
    "# Count architecture types\n",
    "arch_counts = {\n",
    "    \"CNN\": count_by_attribute(\"Convolutional Neural Network\"),\n",
    "    \"Transformer-based\": count_by_attribute(\"Transformer-based Models\"),\n",
    "    \"Vision Transformer\": count_by_attribute(\"Vision Transformer\"),\n",
    "    \"Swin-Transformer\": count_by_attribute(\"Swin-Transformer\"),\n",
    "    \"MLP-Mixer\": count_by_attribute(\"MLP-Mixer\")\n",
    "}\n",
    "\n",
    "# Count multimodal models\n",
    "multimodal_count = count_by_attribute(\"Multimodal\")\n",
    "\n",
    "# Count task types\n",
    "task_counts = {\n",
    "    \"Image Classification\": count_by_attribute(\"Image Classification\"),\n",
    "    \"Object Detection\": count_by_attribute(\"Object Detection\"),\n",
    "    \"Panoptic Segmentation\": count_by_attribute(\"Panoptic Segmentation\"),\n",
    "    \"Semantic Segmentation\": count_by_attribute(\"Semantic Segmentation\"),\n",
    "    \"Instance Segmentation\": count_by_attribute(\"Instance Segmentation\"),\n",
    "    \"Keypoint Detection\": count_by_attribute(\"Keypoint Detection\"),\n",
    "    \"Pose Estimation\": count_by_attribute(\"Pose Estimation\"),\n",
    "    \"Video Classification\": count_by_attribute(\"Video Classification\"),\n",
    "    \"Natural Language Processing\": count_by_attribute(\"Natural Language Processing\"),\n",
    "    \"Audio Tagging\": count_by_attribute(\"Audio Tagging\"),\n",
    "    \"Various visual tasks\": count_by_attribute(\"Various visual tasks\")\n",
    "}\n",
    "\n",
    "# Calculate combined segmentation count\n",
    "seg_columns = [\"Panoptic Segmentation\", \"Semantic Segmentation\", \"Instance Segmentation\"]\n",
    "segmentation_models = set()\n",
    "for col in seg_columns:\n",
    "    # Get indices of models marked for this segmentation type\n",
    "    marked_indices = model_data[model_data[col].apply(is_marked)].index\n",
    "    segmentation_models.update(marked_indices)\n",
    "segmentation_count = len(segmentation_models)\n",
    "\n",
    "# Count training paradigm types\n",
    "paradigm_counts = {\n",
    "    \"Supervised\": count_by_attribute(\"Supervised\"),\n",
    "    \"Jigsaw\": count_by_attribute(\"Jigsaw\"),\n",
    "    \"NPID\": count_by_attribute(\"NPID\"),\n",
    "    \"RotNet\": count_by_attribute(\"RotNet\"),\n",
    "    \"Clusterfit\": count_by_attribute(\"Clusterfit\"),\n",
    "    \"Deepcluster\": count_by_attribute(\"Deepcluser\"),  # Note: typo in column name\n",
    "    \"SimCLR\": count_by_attribute(\"SimCLR\"),\n",
    "    \"SwAV\": count_by_attribute(\"SwAV\"),\n",
    "    \"MoCo\": count_by_attribute(\"MoCo\"),\n",
    "    \"CLIP\": count_by_attribute(\"Contrastive Language Image Pre-Training\")\n",
    "}\n",
    "\n",
    "# Calculate combined self-supervised count\n",
    "ss_methods = [\"Jigsaw\", \"NPID\", \"RotNet\", \"Clusterfit\", \"Deepcluser\", \"SimCLR\", \"SwAV\", \"MoCo\"]\n",
    "ss_models = set()\n",
    "for method in ss_methods:\n",
    "    # Get indices of models marked for this self-supervised method\n",
    "    marked_indices = model_data[model_data[method].apply(is_marked)].index\n",
    "    ss_models.update(marked_indices)\n",
    "self_supervised_count = len(ss_models)\n",
    "\n",
    "# Print counts for debugging\n",
    "print(\"=== ARCHITECTURE COUNTS ===\")\n",
    "for arch, count in arch_counts.items():\n",
    "    print(f\"{arch}: {count}\")\n",
    "print(f\"Multimodal: {multimodal_count}\")\n",
    "\n",
    "print(\"\\n=== TASK COUNTS ===\")\n",
    "for task, count in task_counts.items():\n",
    "    print(f\"{task}: {count}\")\n",
    "print(f\"Combined Segmentation: {segmentation_count}\")\n",
    "\n",
    "print(\"\\n=== TRAINING PARADIGM COUNTS ===\")\n",
    "for paradigm, count in paradigm_counts.items():\n",
    "    print(f\"{paradigm}: {count}\")\n",
    "print(f\"Combined Self-supervised: {self_supervised_count}\")\n",
    "\n",
    "# Print total model count\n",
    "print(f\"\\nTotal models: {len(model_data)}\")\n",
    "\n",
    "# Generate Table 1: Hierarchical Summary Table\n",
    "table1_data = {\n",
    "    \"Architecture Type\": [],\n",
    "    \"Count\": [],\n",
    "    \"Examples\": [],\n",
    "    \"Typical Applications in Neuroscience\": []\n",
    "}\n",
    "\n",
    "neuroscience_applications = {\n",
    "    \"CNN\": \"Visual cortex modeling, object recognition\",\n",
    "    \"Transformer-based\": \"Complex scene understanding, hierarchical processing\",\n",
    "    \"Vision Transformer\": \"Visual attention mechanisms, global feature extraction\",\n",
    "    \"Swin-Transformer\": \"Hierarchical visual processing, local-global integration\",\n",
    "    \"MLP-Mixer\": \"Visual feature extraction without convolutions\",\n",
    "    \"Multimodal\": \"Cross-modal integration studies\"\n",
    "}\n",
    "\n",
    "for arch, count in arch_counts.items():\n",
    "    if count > 0:\n",
    "        table1_data[\"Architecture Type\"].append(arch)\n",
    "        table1_data[\"Count\"].append(count)\n",
    "        \n",
    "        # Get examples based on architecture type\n",
    "        if arch == \"CNN\":\n",
    "            examples = get_example_models(\"Convolutional Neural Network\")\n",
    "        elif arch == \"Transformer-based\":\n",
    "            examples = get_example_models(\"Transformer-based Models\")\n",
    "        elif arch == \"Vision Transformer\":\n",
    "            examples = get_example_models(\"Vision Transformer\")\n",
    "        elif arch == \"Swin-Transformer\":\n",
    "            examples = get_example_models(\"Swin-Transformer\")\n",
    "        elif arch == \"MLP-Mixer\":\n",
    "            examples = get_example_models(\"MLP-Mixer\")\n",
    "        else:\n",
    "            examples = \"N/A\"\n",
    "            \n",
    "        table1_data[\"Examples\"].append(examples)\n",
    "        table1_data[\"Typical Applications in Neuroscience\"].append(neuroscience_applications.get(arch, \"Various visual processes\"))\n",
    "\n",
    "# Add Multimodal as a separate entry\n",
    "if multimodal_count > 0:\n",
    "    table1_data[\"Architecture Type\"].append(\"Multimodal\")\n",
    "    table1_data[\"Count\"].append(multimodal_count)\n",
    "    table1_data[\"Examples\"].append(get_example_models(\"Multimodal\"))\n",
    "    table1_data[\"Typical Applications in Neuroscience\"].append(neuroscience_applications[\"Multimodal\"])\n",
    "\n",
    "table1 = pd.DataFrame(table1_data)\n",
    "\n",
    "# Generate Table 2: Training Paradigm Distribution\n",
    "table2_data = {\n",
    "    \"Training Paradigm\": [\n",
    "        \"Supervised\", \n",
    "        \"Self-supervised\", \n",
    "        \"Multimodal (CLIP)\",\n",
    "        \"Task-specific\"\n",
    "    ],\n",
    "    \"Dataset Examples\": [\n",
    "        \"ImageNet, COCO\",\n",
    "        \"SimCLR, MoCo, SwAV\",\n",
    "        \"CLIP (image-text)\",\n",
    "        \"Object detection, segmentation\"\n",
    "    ],\n",
    "    \"Count\": [\n",
    "        paradigm_counts[\"Supervised\"],\n",
    "        self_supervised_count,\n",
    "        paradigm_counts[\"CLIP\"],\n",
    "        task_counts[\"Object Detection\"] + segmentation_count\n",
    "    ],\n",
    "    \"Neuroscientific Relevance\": [\n",
    "        \"Comparison to human category learning\",\n",
    "        \"Unsupervised feature representation similar to developmental processes\",\n",
    "        \"Cross-modal integration studies\",\n",
    "        \"Specialized visual processing pathways\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Add individual self-supervised methods with non-zero counts\n",
    "for method in ss_methods:\n",
    "    method_key = method if method != \"Deepcluster\" else \"Deepcluster\"  # Handle naming inconsistency\n",
    "    if paradigm_counts.get(method_key, 0) > 0:\n",
    "        table2_data[\"Training Paradigm\"].append(f\"  {method}\")  # Indented to show hierarchy\n",
    "        table2_data[\"Dataset Examples\"].append(\"\")\n",
    "        table2_data[\"Count\"].append(paradigm_counts[method_key])\n",
    "        table2_data[\"Neuroscientific Relevance\"].append(\"\")\n",
    "\n",
    "table2 = pd.DataFrame(table2_data)\n",
    "\n",
    "# Generate Table 3: Task-Based Selection Criteria\n",
    "table3_data = {\n",
    "    \"Task Category\": [],\n",
    "    \"# Models\": [],\n",
    "    \"Key Architectures\": [],\n",
    "    \"Neuroscientific Application\": []\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "task_neuroscience_mapping = {\n",
    "    \"Image Classification\": \"Ventral visual stream, object recognition\",\n",
    "    \"Object Detection\": \"Attentional mechanisms, object localization\",\n",
    "    \"Segmentation\": \"Scene parsing, figure-ground segregation\",\n",
    "    \"Video Classification\": \"Motion processing, temporal integration\",\n",
    "    \"Audio Tagging\": \"Auditory processing pathways\",\n",
    "    \"Keypoint Detection\": \"Biological motion perception\",\n",
    "    \"Pose Estimation\": \"Action recognition, motor system modeling\",\n",
    "    \"Natural Language Processing\":\"?\",\n",
    "    \"Various visual tasks\":\"other\"\n",
    "    \n",
    "}\n",
    "\n",
    "# Add all tasks with non-zero counts\n",
    "for task, count in task_counts.items():\n",
    "    if count > 0:\n",
    "        table3_data[\"Task Category\"].append(task)\n",
    "        table3_data[\"# Models\"].append(count)\n",
    "        table3_data[\"Key Architectures\"].append(get_example_models(task))\n",
    "        \n",
    "        # Map to neuroscience application\n",
    "        if task in [\"Panoptic Segmentation\", \"Semantic Segmentation\", \"Instance Segmentation\"]:\n",
    "            neuro_app = task_neuroscience_mapping[\"Segmentation\"]\n",
    "        else:\n",
    "            neuro_app = task_neuroscience_mapping.get(task, \"Various neural processes\")\n",
    "            \n",
    "        table3_data[\"Neuroscientific Application\"].append(neuro_app)\n",
    "\n",
    "# Add combined segmentation entry if it has a count\n",
    "if segmentation_count > 0:\n",
    "    table3_data[\"Task Category\"].append(\"All Segmentation Tasks\")\n",
    "    table3_data[\"# Models\"].append(segmentation_count)\n",
    "    \n",
    "    # Get examples from different segmentation types\n",
    "    seg_examples = []\n",
    "    for seg_type in [\"Panoptic Segmentation\", \"Semantic Segmentation\", \"Instance Segmentation\"]:\n",
    "        examples = get_example_models(seg_type)\n",
    "        if examples != \"N/A\":\n",
    "            seg_examples.append(examples)\n",
    "    \n",
    "    table3_data[\"Key Architectures\"].append(\", \".join(seg_examples) if seg_examples else \"Mask R-CNN, DeepLab, UNet\")\n",
    "    table3_data[\"Neuroscientific Application\"].append(task_neuroscience_mapping[\"Segmentation\"])\n",
    "\n",
    "table3 = pd.DataFrame(table3_data)\n",
    "\n",
    "# Print tables\n",
    "print(\"\\n===== TABLE 1: Hierarchical Summary Table =====\")\n",
    "print(table1.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n===== TABLE 2: Training Paradigm Distribution =====\")\n",
    "print(table2.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n===== TABLE 3: Task-Based Selection Criteria =====\")\n",
    "print(table3.to_string(index=False))\n",
    "\n",
    "# Generate LaTeX tables\n",
    "def generate_latex_table(df, caption):\n",
    "    if len(df) == 0:\n",
    "        return f\"% Empty table for {caption}\"\n",
    "    \n",
    "    latex = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\"\n",
    "    latex += f\"\\\\caption{{{caption}}}\\n\"\n",
    "    latex += \"\\\\begin{tabular}{\" + \"l\" * len(df.columns) + \"}\\n\"\n",
    "    latex += \"\\\\toprule\\n\"\n",
    "    \n",
    "    # Headers\n",
    "    latex += \" & \".join(df.columns) + \" \\\\\\\\\\n\"\n",
    "    latex += \"\\\\midrule\\n\"\n",
    "    \n",
    "    # Rows\n",
    "    for _, row in df.iterrows():\n",
    "        latex += \" & \".join([str(val) for val in row.values]) + \" \\\\\\\\\\n\"\n",
    "    \n",
    "    latex += \"\\\\bottomrule\\n\"\n",
    "    latex += \"\\\\end{tabular}\\n\"\n",
    "    latex += \"\\\\end{table}\\n\"\n",
    "    \n",
    "    return latex\n",
    "\n",
    "latex1 = generate_latex_table(table1, \"Distribution of models by architecture type\")\n",
    "latex2 = generate_latex_table(table2, \"Distribution of models by training paradigm\")\n",
    "latex3 = generate_latex_table(table3, \"Distribution of models by task category\")\n",
    "\n",
    "print(\"\\n\\n===== LATEX TABLE 1 =====\")\n",
    "print(latex1)\n",
    "\n",
    "print(\"\\n===== LATEX TABLE 2 =====\")\n",
    "print(latex2)\n",
    "\n",
    "print(\"\\n===== LATEX TABLE 3 =====\")\n",
    "print(latex3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total models in taxonomy: 649\n",
      "\n",
      "=== ARCHITECTURE ANALYSIS ===\n",
      "CNN: 522 models\n",
      "Transformer-based: 13 models\n",
      "Vision Transformer: 80 models\n",
      "Swin-Transformer: 10 models\n",
      "MLP-Mixer: 25 models\n",
      "Multimodal: 5 models\n",
      "\n",
      "Models with multiple architecture labels: 15\n",
      "Models with at least one architecture label: 640\n",
      "Models without any architecture label: 9\n",
      "\n",
      "Examples of models with multiple architecture labels:\n",
      "  - efficientnet_b4: CNN\n",
      "  - efficientnet_b0: CNN\n",
      "  - efficientnet_b1: CNN\n",
      "  - wide_resnet101_2: CNN\n",
      "  - RN50: CNN, Multimodal\n",
      "  - ViT-B_-_16: Vision Transformer, Multimodal\n",
      "  - resnext50_32x4d: CNN\n",
      "  - resnext101_32x8d: CNN\n",
      "  - ViT-L_-_14: Vision Transformer, Multimodal\n",
      "  - adv_inception_v3: CNN, Vision Transformer\n",
      "\n",
      "Examples of models without architecture labels:\n",
      "\n",
      "\n",
      "=== TASK ANALYSIS ===\n",
      "Image Classification: 520 models\n",
      "Various visual tasks: 23 models\n",
      "Object Detection: 32 models\n",
      "Panoptic Segmentation: 4 models\n",
      "Semantic Segmentation: 1 models\n",
      "Instance Segmentation: 24 models\n",
      "Keypoint Detection: 4 models\n",
      "Pose Estimation: 0 models\n",
      "Video Classification: 6 models\n",
      "Natural Language Processing: 6 models\n",
      "Audio Tagging: 29 models\n",
      "\n",
      "Models with multiple task labels: 9\n",
      "Models with at least one task label: 640\n",
      "Models without any task label: 9\n",
      "\n",
      "Examples of models with multiple task labels:\n",
      "  - efficientnet_b4: Image Classification\n",
      "  - efficientnet_b0: Image Classification\n",
      "  - efficientnet_b1: Image Classification\n",
      "  - wide_resnet101_2: Image Classification\n",
      "  - resnext50_32x4d: Image Classification\n",
      "  - resnext101_32x8d: Image Classification\n",
      "  - wide_resnet50_2: Image Classification\n",
      "  - efficientnet_b3: Image Classification\n",
      "  - efficientnet_b2: Image Classification\n",
      "\n",
      "Examples of models without task labels:\n",
      "\n",
      "\n",
      "=== SEGMENTATION TASKS ANALYSIS ===\n",
      "Panoptic Segmentation models: 4\n",
      "Semantic Segmentation models: 1\n",
      "Instance Segmentation models: 24\n",
      "Combined unique segmentation models: 29\n",
      "Models in multiple segmentation categories: 0\n",
      "\n",
      "\n",
      "=== CROSS-ANALYSIS: ARCHITECTURE VS TASK ===\n",
      "\n",
      "Models with both architecture and task labels: 640\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('net2brain/architectures/taxonomy.csv')\n",
    "# Identify where actual data begins (after header rows)\n",
    "data_start = 1  # Based on your sample, data starts from row index 1\n",
    "\n",
    "# Get the actual data rows\n",
    "model_data = data.iloc[data_start:].copy()\n",
    "\n",
    "# Function to check if a value represents an 'x' marking\n",
    "def is_marked(val):\n",
    "    if pd.isna(val):\n",
    "        return False\n",
    "    return str(val).lower() == 'x'\n",
    "\n",
    "# Count total models\n",
    "total_models = len(model_data)\n",
    "print(f\"Total models in taxonomy: {total_models}\")\n",
    "\n",
    "# ARCHITECTURE ANALYSIS\n",
    "print(\"\\n=== ARCHITECTURE ANALYSIS ===\")\n",
    "\n",
    "# Create a dictionary to track which models belong to each architecture type\n",
    "arch_models = {\n",
    "    \"CNN\": [],\n",
    "    \"Transformer-based\": [],\n",
    "    \"Vision Transformer\": [],\n",
    "    \"Swin-Transformer\": [],\n",
    "    \"MLP-Mixer\": [],\n",
    "    \"Multimodal\": []\n",
    "}\n",
    "\n",
    "# Collect models for each architecture type\n",
    "for idx, row in model_data.iterrows():\n",
    "    model_name = row['Model']\n",
    "    \n",
    "    if is_marked(row['Convolutional Neural Network']):\n",
    "        arch_models[\"CNN\"].append(model_name)\n",
    "    \n",
    "    if is_marked(row['Transformer-based Models']):\n",
    "        arch_models[\"Transformer-based\"].append(model_name)\n",
    "    \n",
    "    if is_marked(row['Vision Transformer']):\n",
    "        arch_models[\"Vision Transformer\"].append(model_name)\n",
    "    \n",
    "    if is_marked(row['Swin-Transformer']):\n",
    "        arch_models[\"Swin-Transformer\"].append(model_name)\n",
    "    \n",
    "    if is_marked(row['MLP-Mixer']):\n",
    "        arch_models[\"MLP-Mixer\"].append(model_name)\n",
    "    \n",
    "    if is_marked(row['Multimodal']):\n",
    "        arch_models[\"Multimodal\"].append(model_name)\n",
    "\n",
    "# Print architecture model counts\n",
    "for arch, models in arch_models.items():\n",
    "    print(f\"{arch}: {len(models)} models\")\n",
    "\n",
    "# Find models with multiple architecture labels\n",
    "all_arch_models = set()\n",
    "multi_arch_models = set()\n",
    "\n",
    "for arch, models in arch_models.items():\n",
    "    for model in models:\n",
    "        if model in all_arch_models:\n",
    "            multi_arch_models.add(model)\n",
    "        all_arch_models.add(model)\n",
    "\n",
    "# Calculate models with architecture labels and models without\n",
    "models_with_arch = len(all_arch_models)\n",
    "models_without_arch = total_models - models_with_arch\n",
    "\n",
    "print(f\"\\nModels with multiple architecture labels: {len(multi_arch_models)}\")\n",
    "print(f\"Models with at least one architecture label: {models_with_arch}\")\n",
    "print(f\"Models without any architecture label: {models_without_arch}\")\n",
    "\n",
    "# Print examples of models with multiple architecture labels\n",
    "if multi_arch_models:\n",
    "    print(\"\\nExamples of models with multiple architecture labels:\")\n",
    "    for model in list(multi_arch_models)[:10]:  # Show up to 10 examples\n",
    "        archs = [arch for arch, models in arch_models.items() if model in models]\n",
    "        print(f\"  - {model}: {', '.join(archs)}\")\n",
    "\n",
    "# Print examples of models without architecture labels\n",
    "if models_without_arch > 0:\n",
    "    models_without_arch_list = [row['Model'] for idx, row in model_data.iterrows() \n",
    "                               if row['Model'] not in all_arch_models]\n",
    "    print(\"\\nExamples of models without architecture labels:\")\n",
    "    for model in models_without_arch_list:  # Show up to 10 examples\n",
    "        print(f\"  - {model}\")\n",
    "\n",
    "# TASK ANALYSIS\n",
    "print(\"\\n\\n=== TASK ANALYSIS ===\")\n",
    "\n",
    "# Create a dictionary to track which models belong to each task\n",
    "task_models = {\n",
    "    \"Image Classification\": [],\n",
    "    \"Various visual tasks\": [],\n",
    "    \"Object Detection\": [],\n",
    "    \"Panoptic Segmentation\": [],\n",
    "    \"Semantic Segmentation\": [],\n",
    "    \"Instance Segmentation\": [],\n",
    "    \"Keypoint Detection\": [],\n",
    "    \"Pose Estimation\": [],\n",
    "    \"Video Classification\": [],\n",
    "    \"Natural Language Processing\": [],\n",
    "    \"Audio Tagging\": []\n",
    "}\n",
    "\n",
    "# Collect models for each task\n",
    "for idx, row in model_data.iterrows():\n",
    "    model_name = row['Model']\n",
    "    \n",
    "    if is_marked(row['Image Classification']):\n",
    "        task_models[\"Image Classification\"].append(model_name)\n",
    "    \n",
    "    if is_marked(row['Object Detection']):\n",
    "        task_models[\"Object Detection\"].append(model_name)\n",
    "    \n",
    "    if is_marked(row['Panoptic Segmentation']):\n",
    "        task_models[\"Panoptic Segmentation\"].append(model_name)\n",
    "    \n",
    "    if is_marked(row['Semantic Segmentation']):\n",
    "        task_models[\"Semantic Segmentation\"].append(model_name)\n",
    "    \n",
    "    if is_marked(row['Instance Segmentation']):\n",
    "        task_models[\"Instance Segmentation\"].append(model_name)\n",
    "    \n",
    "    if is_marked(row['Keypoint Detection']):\n",
    "        task_models[\"Keypoint Detection\"].append(model_name)\n",
    "    \n",
    "    if is_marked(row['Pose Estimation']):\n",
    "        task_models[\"Pose Estimation\"].append(model_name)\n",
    "    \n",
    "    if is_marked(row['Video Classification']):\n",
    "        task_models[\"Video Classification\"].append(model_name)\n",
    "    \n",
    "    if is_marked(row['Audio Tagging']):\n",
    "        task_models[\"Audio Tagging\"].append(model_name)\n",
    "        \n",
    "    if is_marked(row[\"Various visual tasks\"]):\n",
    "        task_models[\"Various visual tasks\"].append(model_name)\n",
    "        \n",
    "    if is_marked(row[\"Natural Language Processing\"]):\n",
    "        task_models[\"Natural Language Processing\"].append(model_name)\n",
    "\n",
    "# Print task model counts\n",
    "for task, models in task_models.items():\n",
    "    print(f\"{task}: {len(models)} models\")\n",
    "\n",
    "# Find models with multiple task labels\n",
    "all_task_models = set()\n",
    "multi_task_models = set()\n",
    "\n",
    "for task, models in task_models.items():\n",
    "    for model in models:\n",
    "        if model in all_task_models:\n",
    "            multi_task_models.add(model)\n",
    "        all_task_models.add(model)\n",
    "\n",
    "# Calculate models with task labels and models without\n",
    "models_with_task = len(all_task_models)\n",
    "models_without_task = total_models - models_with_task\n",
    "\n",
    "print(f\"\\nModels with multiple task labels: {len(multi_task_models)}\")\n",
    "print(f\"Models with at least one task label: {models_with_task}\")\n",
    "print(f\"Models without any task label: {models_without_task}\")\n",
    "\n",
    "# Print examples of models with multiple task labels\n",
    "if multi_task_models:\n",
    "    print(\"\\nExamples of models with multiple task labels:\")\n",
    "    for model in list(multi_task_models)[:10]:  # Show up to 10 examples\n",
    "        tasks = [task for task, models in task_models.items() if model in models]\n",
    "        print(f\"  - {model}: {', '.join(tasks)}\")\n",
    "\n",
    "# Print examples of models without task labels\n",
    "if models_without_task > 0:\n",
    "    models_without_task_list = [row['Model'] for idx, row in model_data.iterrows() \n",
    "                               if row['Model'] not in all_task_models]\n",
    "    print(\"\\nExamples of models without task labels:\")\n",
    "    for model in models_without_task_list:  # Show up to 10 examples\n",
    "        print(f\"  - {model}\")\n",
    "\n",
    "# SEGMENTATION ANALYSIS\n",
    "print(\"\\n\\n=== SEGMENTATION TASKS ANALYSIS ===\")\n",
    "\n",
    "# Create sets for each segmentation task\n",
    "panoptic_models = set(task_models[\"Panoptic Segmentation\"])\n",
    "semantic_models = set(task_models[\"Semantic Segmentation\"])\n",
    "instance_models = set(task_models[\"Instance Segmentation\"])\n",
    "\n",
    "# Find the combined set (models that do any segmentation task)\n",
    "all_seg_models = panoptic_models.union(semantic_models).union(instance_models)\n",
    "\n",
    "# Check for models that appear in multiple segmentation categories\n",
    "multi_seg_models = set()\n",
    "\n",
    "for model in panoptic_models:\n",
    "    if model in semantic_models or model in instance_models:\n",
    "        multi_seg_models.add(model)\n",
    "\n",
    "for model in semantic_models:\n",
    "    if model in instance_models:\n",
    "        multi_seg_models.add(model)\n",
    "\n",
    "print(f\"Panoptic Segmentation models: {len(panoptic_models)}\")\n",
    "print(f\"Semantic Segmentation models: {len(semantic_models)}\")\n",
    "print(f\"Instance Segmentation models: {len(instance_models)}\")\n",
    "print(f\"Combined unique segmentation models: {len(all_seg_models)}\")\n",
    "print(f\"Models in multiple segmentation categories: {len(multi_seg_models)}\")\n",
    "\n",
    "if multi_seg_models:\n",
    "    print(\"\\nModels that appear in multiple segmentation categories:\")\n",
    "    for model in multi_seg_models:\n",
    "        categories = []\n",
    "        if model in panoptic_models:\n",
    "            categories.append(\"Panoptic\")\n",
    "        if model in semantic_models:\n",
    "            categories.append(\"Semantic\")\n",
    "        if model in instance_models:\n",
    "            categories.append(\"Instance\")\n",
    "        print(f\"  - {model}: {', '.join(categories)}\")\n",
    "\n",
    "# CROSS-ANALYSIS BETWEEN ARCHITECTURE AND TASK\n",
    "print(\"\\n\\n=== CROSS-ANALYSIS: ARCHITECTURE VS TASK ===\")\n",
    "\n",
    "# Find models that have architecture label but no task label\n",
    "arch_but_no_task = all_arch_models - all_task_models\n",
    "if arch_but_no_task:\n",
    "    print(f\"Models with architecture label but no task label: {len(arch_but_no_task)}\")\n",
    "    print(\"Examples:\")\n",
    "    for model in list(arch_but_no_task)[:10]:\n",
    "        archs = [arch for arch, models in arch_models.items() if model in models]\n",
    "        print(f\"  - {model}: {', '.join(archs)}\")\n",
    "\n",
    "# Find models that have task label but no architecture label\n",
    "task_but_no_arch = all_task_models - all_arch_models\n",
    "if task_but_no_arch:\n",
    "    print(f\"\\nModels with task label but no architecture label: {len(task_but_no_arch)}\")\n",
    "    print(\"Examples:\")\n",
    "    for model in list(task_but_no_arch)[:10]:\n",
    "        tasks = [task for task, models in task_models.items() if model in models]\n",
    "        print(f\"  - {model}: {', '.join(tasks)}\")\n",
    "\n",
    "# Find models with both architecture and task labels\n",
    "models_with_both = all_arch_models.intersection(all_task_models)\n",
    "print(f\"\\nModels with both architecture and task labels: {len(models_with_both)}\")\n",
    "\n",
    "# Find models without any labels\n",
    "models_without_any = set(model_data['Model']) - all_arch_models - all_task_models\n",
    "if models_without_any:\n",
    "    print(f\"\\nModels without any architecture or task labels: {len(models_without_any)}\")\n",
    "    print(\"Examples:\")\n",
    "    for model in list(models_without_any)[:10]:\n",
    "        print(f\"  - {model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "N2B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
